{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ab09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import explode_outer, explode\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24846173",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").appName(\"COMP5349 --- 470418978\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921fe85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbe36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=spark.read.option(\"multiline\",\"true\").json(\"tweets.json\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06076a10",
   "metadata": {},
   "source": [
    "## Workload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6c9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_w1=tweets.filter(\"replyto_id IS NOT NULL OR retweet_id IS NOT NULL\").select('user_id', f.concat_ws('-', 'replyto_id','retweet_id').alias(\"tweet_id\")).groupBy(\"user_id\").agg(f.concat_ws(\", \", f.collect_list(\"tweet_id\")).alias(\"ids\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06cafbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer\n",
    "from pyspark.ml.feature import CountVectorizer, Word2Vec\n",
    "\n",
    "\n",
    "## Workload 1 - CountVectorizer\n",
    "tokenizer = Tokenizer(inputCol=\"ids\", outputCol=\"ids_tokenized\")\n",
    "tokenized_data = tokenizer.transform(tweets_w1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703ff34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(inputCol='ids_tokenized', outputCol=\"cvfeatures\", vocabSize=200, minDF=1)\n",
    "cvmodel=cv.fit(tokenized_data)\n",
    "cv_data=cvmodel.transform(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1816dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"ids_tokenized\", outputCol=\"rawFeatures\", numFeatures=200)\n",
    "featurized_data = hashingTF.transform(cv_data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0b43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe3098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer=Normalizer(inputCol='features',outputCol='features_norm', p=2.0)\n",
    "normalized_data=normalizer.transform(rescaled_data)\n",
    "\n",
    "normalizer_cv=Normalizer(inputCol='cvfeatures', outputCol='cvfeatures_norm', p=2.0)\n",
    "normalized_data=normalizer_cv.transform(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12599492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_partition(list_of_records):\n",
    "    final_iterator=[]\n",
    "    for record in list_of_records:\n",
    "        user_id=record[\"user_id\"]\n",
    "        vector_tfidf=record[\"features_norm\"]\n",
    "        vector_cv=record[\"cvfeatures_norm\"]\n",
    "        final_iterator.append((user_id,(vector_tfidf,vector_cv)))\n",
    "    return iter(final_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6956445",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_both=normalized_data.select('user_id','features_norm','cvfeatures_norm').rdd.mapPartitions(reformat_partition).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7446664",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_lookup_broadcast=sc.broadcast(extracted_features_both.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5944e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_info(record):\n",
    "    vector_info=vector_lookup_broadcast.value.get(record)\n",
    "    \n",
    "    tf_sim_list=[]\n",
    "    cv_sim_list=[]\n",
    "    \n",
    "    for key,value in vector_lookup_broadcast.value.items():\n",
    "        if key != record:\n",
    "            temp_user_id=key\n",
    "            temp_vector_info=value\n",
    "    \n",
    "            tf_sim=vector_info[0].dot(temp_vector_info[0])\n",
    "            cv_sim=vector_info[1].dot(temp_vector_info[1])\n",
    "        \n",
    "            tf_sim_list.append((tf_sim,temp_user_id))\n",
    "            cv_sim_list.append((cv_sim,temp_user_id))\n",
    "        \n",
    "        \n",
    "    return(record, (tf_sim_list,cv_sim_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34368e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id=202170318\n",
    "output_rdd=spark.sparkContext.parallelize([test_id]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81220bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(202170318,\n",
       "  ([(0.5082028314384416, 1016523579205222401),\n",
       "    (0.5082028314384416, 53105096),\n",
       "    (0.48442447933608124, 138309569),\n",
       "    (0.47967493198551064, 4879483186),\n",
       "    (0.47967493198551064, 1466023136)],\n",
       "   [(0.5163977794943223, 1323090359581200385),\n",
       "    (0.5163977794943223, 138309569),\n",
       "    (0.4472135954999579, 1343557199394525186),\n",
       "    (0.4472135954999579, 1234011788401725440),\n",
       "    (0.4472135954999579, 1030305416771764224)]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rdd.map(get_vector_info).mapValues(lambda x: (sorted(x[0], reverse=True)[:5], sorted(x[1], reverse=True)[:5])).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96a607",
   "metadata": {},
   "source": [
    "## Workload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10678cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_w2=tweets.select(\"user_id\",explode(\"user_mentions\")).filter(\"user_id IS NOT NULL\").withColumn(\"col\",f.col(\"col\")[\"id\"])\n",
    "pair_rating=tweets_w2.groupBy(tweets_w2.columns).count().select('user_id', f.hash('user_id').alias('hash_user_id'), f.col(\"col\").alias('mention_user_id'), f.hash('col').alias('hash_mention_id'), f.col('count').alias('y')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d7d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_w2_partition(list_of_records):\n",
    "    final_iterator=[]\n",
    "    for record in list_of_records:\n",
    "        user_id=record[0]\n",
    "        hash_user_id=record[1]\n",
    "        mention_user_id=record[2]\n",
    "        hash_mention_user_id=record[3]\n",
    "        final_iterator.append((user_id, hash_user_id, mention_user_id, hash_mention_user_id))\n",
    "    return iter(final_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a34c1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_values_w2(record):\n",
    "    try:\n",
    "        id1=record[0][0]\n",
    "        id2=record[1][0]\n",
    "        id3=record[2][0]\n",
    "        id4=record[3][0]\n",
    "        id5=record[4][0]\n",
    "        return(id1, id2, id3, id4, id5)\n",
    "    except:\n",
    "        return ()\n",
    "\n",
    "def convert_mention(mention_hashes):\n",
    "    try:\n",
    "        return_list=[]\n",
    "        for mention_hash in mention_hashes:\n",
    "            for value in id_hash_map.value:\n",
    "                if value[3] == mention_hash:\n",
    "                    mention_id=value[2]\n",
    "                    return_list.append(mention_id)\n",
    "                    break\n",
    "        return return_list\n",
    "    except:\n",
    "        return()\n",
    "\n",
    "\n",
    "def convert_user(user_hash):\n",
    "    try:\n",
    "        for value in id_hash_map.value:\n",
    "            if value[1] == user_hash:\n",
    "                return value[0]\n",
    "    except:\n",
    "        return ()\n",
    "\n",
    "\n",
    "def convert_hash(record):\n",
    "    try:\n",
    "        user_hash=record[0]\n",
    "        mention_hashes=record[1]\n",
    "\n",
    "        user_id=convert_user(user_hash)\n",
    "\n",
    "        mention_ids=convert_mention(mention_hashes)\n",
    "\n",
    "        return(user_id, mention_ids)\n",
    "\n",
    "    except:\n",
    "        return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "835237d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_hash_map=sc.broadcast(pair_rating.select(\"user_id\",\"hash_user_id\",\"mention_user_id\", \"hash_mention_id\").rdd.mapPartitions(reformat_w2_partition).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f4133ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ALS(rank=10, seed=0, maxIter=5, regParam=0.1, implicitPrefs=True, alpha=1.0, userCol='hash_user_id', itemCol='hash_mention_id', ratingCol='y').fit(pair_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a2aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_subset=pair_rating.select(\"hash_user_id\").distinct()\n",
    "top_predictions=model.recommendForUserSubset(user_subset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70a86fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_predictions_rdd=top_predictions.rdd.mapValues(reformat_values_w2).map(convert_hash)\n",
    "top_predictions_rdd.saveAsTextFile('recommendations_final_7.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
